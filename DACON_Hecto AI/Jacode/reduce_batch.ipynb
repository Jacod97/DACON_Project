{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뭐부터 할까\n",
    "- dateset Normalize 평균과 분산 확인하기 \n",
    "    - base코드와 많이 다르면 새로 구한 평균과 분산 사용해서 Normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "CFG = {\n",
    "    'IMG_SIZE': 224,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 20,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'SEED' : 42\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path, transform=None, is_test=False):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if is_test:\n",
    "            # 테스트셋: 라벨 없이 이미지 경로만 저장\n",
    "            for fname in sorted(os.listdir(path)):\n",
    "                if fname.lower().endswith(('.jpg')):\n",
    "                    img_path = os.path.join(path, fname)\n",
    "                    self.samples.append((img_path,))\n",
    "        else:\n",
    "            # 학습셋: 클래스별 폴더 구조에서 라벨 추출\n",
    "            self.classes = sorted(os.listdir(path))\n",
    "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "            for cls_name in self.classes:\n",
    "                cls_folder = os.path.join(path, cls_name)\n",
    "                for fname in os.listdir(cls_folder):\n",
    "                    if fname.lower().endswith(('.jpg')):\n",
    "                        img_path = os.path.join(cls_folder, fname)\n",
    "                        label = self.class_to_idx[cls_name]\n",
    "                        self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            img_path = self.samples[idx][0]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "        else:\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "\n",
    "train_root = '../data/train'\n",
    "test_root = '../data/test'\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# companys = [\n",
    "#     \"HYUNDAI\",\"KIA\",\"GENESIS\",\"BMW\",\"Chevrolet\"\n",
    "#     \"PEUGEOT\",\"PORSCHE\",\"BENZ\",\"AUDI\",\"KG\"\n",
    "#     \"JAGUAR\",\"NISSAN\",\"LEXUS\",\"etc\",\"VOLVO\"\n",
    "#     \"LINCOLN\", \"RENAULT\", \"TOYOTA\", \"RANGE ROVER\"L\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 이미지 수: 33137\n",
      "train 이미지 수: 26509, valid 이미지 수: 6628\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 전체 데이터셋 로드\n",
    "full_dataset = CustomDataset(train_root, transform=None)\n",
    "print(f\"총 이미지 수: {len(full_dataset)}\")\n",
    "\n",
    "targets = [label for _, label in full_dataset.samples]\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "# Stratified Split\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(targets)), test_size=0.2, stratify=targets, random_state=42\n",
    ")\n",
    "\n",
    "# Subset + transform 각각 적용\n",
    "train_dataset = Subset(CustomDataset(train_root, transform=train_transform), train_idx)\n",
    "val_dataset = Subset(CustomDataset(train_root, transform=val_transform), val_idx)\n",
    "print(f'train 이미지 수: {len(train_dataset)}, valid 이미지 수: {len(val_dataset)}')\n",
    "\n",
    "\n",
    "# DataLoader 정의\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)  # ResNet18 모델 불러오기\n",
    "        self.feature_dim = self.backbone.fc.in_features \n",
    "        self.backbone.fc = nn.Identity()  # feature extractor로만 사용\n",
    "        self.head = nn.Linear(self.feature_dim, num_classes)  # 분류기\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)       \n",
    "        x = self.head(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "[Epoch 1/20] Training: 100%|██████████| 829/829 [12:20<00:00,  1.12it/s]\n",
      "[Epoch 1/20] Validation: 100%|██████████| 208/208 [01:23<00:00,  2.48it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 3.6297 || Valid Loss : 1.4838 | Valid Accuracy : 77.7158%\n",
      "📦 Best model saved at epoch 1 (logloss: 1.4856)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2/20] Training: 100%|██████████| 829/829 [12:32<00:00,  1.10it/s]\n",
      "[Epoch 2/20] Validation: 100%|██████████| 208/208 [01:22<00:00,  2.51it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.8292 || Valid Loss : 0.5101 | Valid Accuracy : 88.8805%\n",
      "📦 Best model saved at epoch 2 (logloss: 0.5097)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3/20] Training: 100%|██████████| 829/829 [12:37<00:00,  1.09it/s]\n",
      "[Epoch 3/20] Validation: 100%|██████████| 208/208 [01:22<00:00,  2.52it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.2534 || Valid Loss : 0.3293 | Valid Accuracy : 92.0489%\n",
      "📦 Best model saved at epoch 3 (logloss: 0.3296)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4/20] Training: 100%|██████████| 829/829 [12:38<00:00,  1.09it/s]\n",
      "[Epoch 4/20] Validation: 100%|██████████| 208/208 [01:22<00:00,  2.51it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.1093 || Valid Loss : 0.2811 | Valid Accuracy : 93.0748%\n",
      "📦 Best model saved at epoch 4 (logloss: 0.2817)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5/20] Training: 100%|██████████| 829/829 [12:41<00:00,  1.09it/s]\n",
      "[Epoch 5/20] Validation: 100%|██████████| 208/208 [01:24<00:00,  2.45it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0614 || Valid Loss : 0.2661 | Valid Accuracy : 93.1804%\n",
      "📦 Best model saved at epoch 5 (logloss: 0.2669)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6/20] Training: 100%|██████████| 829/829 [12:41<00:00,  1.09it/s]\n",
      "[Epoch 6/20] Validation: 100%|██████████| 208/208 [01:24<00:00,  2.47it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0402 || Valid Loss : 0.2833 | Valid Accuracy : 93.1654%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 7/20] Training: 100%|██████████| 829/829 [12:43<00:00,  1.09it/s]\n",
      "[Epoch 7/20] Validation: 100%|██████████| 208/208 [01:24<00:00,  2.46it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0389 || Valid Loss : 0.3528 | Valid Accuracy : 91.1889%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 8/20] Training: 100%|██████████| 829/829 [12:40<00:00,  1.09it/s]\n",
      "[Epoch 8/20] Validation: 100%|██████████| 208/208 [01:23<00:00,  2.48it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0513 || Valid Loss : 0.2806 | Valid Accuracy : 92.3808%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 9/20] Training: 100%|██████████| 829/829 [12:41<00:00,  1.09it/s]\n",
      "[Epoch 9/20] Validation: 100%|██████████| 208/208 [01:24<00:00,  2.47it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0295 || Valid Loss : 0.3194 | Valid Accuracy : 91.7019%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 10/20] Training: 100%|██████████| 829/829 [12:40<00:00,  1.09it/s]\n",
      "[Epoch 10/20] Validation: 100%|██████████| 208/208 [01:25<00:00,  2.43it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0285 || Valid Loss : 0.2775 | Valid Accuracy : 92.7580%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 11/20] Training: 100%|██████████| 829/829 [12:38<00:00,  1.09it/s]\n",
      "[Epoch 11/20] Validation: 100%|██████████| 208/208 [01:22<00:00,  2.51it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0292 || Valid Loss : 0.3021 | Valid Accuracy : 92.3808%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 12/20] Training: 100%|██████████| 829/829 [12:38<00:00,  1.09it/s]\n",
      "[Epoch 12/20] Validation: 100%|██████████| 208/208 [01:23<00:00,  2.48it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0323 || Valid Loss : 0.2800 | Valid Accuracy : 93.1352%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 13/20] Training: 100%|██████████| 829/829 [12:42<00:00,  1.09it/s]\n",
      "[Epoch 13/20] Validation: 100%|██████████| 208/208 [01:24<00:00,  2.48it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0173 || Valid Loss : 0.3091 | Valid Accuracy : 91.9282%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 14/20] Training: 100%|██████████| 829/829 [12:34<00:00,  1.10it/s]\n",
      "[Epoch 14/20] Validation: 100%|██████████| 208/208 [01:24<00:00,  2.47it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0310 || Valid Loss : 0.2657 | Valid Accuracy : 93.3011%\n",
      "📦 Best model saved at epoch 14 (logloss: 0.2668)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 15/20] Training: 100%|██████████| 829/829 [12:39<00:00,  1.09it/s]\n",
      "[Epoch 15/20] Validation: 100%|██████████| 208/208 [01:24<00:00,  2.47it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0172 || Valid Loss : 0.2443 | Valid Accuracy : 93.4671%\n",
      "📦 Best model saved at epoch 15 (logloss: 0.2453)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 16/20] Training: 100%|██████████| 829/829 [12:34<00:00,  1.10it/s]\n",
      "[Epoch 16/20] Validation: 100%|██████████| 208/208 [01:22<00:00,  2.52it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0204 || Valid Loss : 0.2841 | Valid Accuracy : 92.8787%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 17/20] Training: 100%|██████████| 829/829 [12:29<00:00,  1.11it/s]\n",
      "[Epoch 17/20] Validation: 100%|██████████| 208/208 [01:21<00:00,  2.54it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0300 || Valid Loss : 0.2519 | Valid Accuracy : 93.4520%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 18/20] Training: 100%|██████████| 829/829 [12:30<00:00,  1.10it/s]\n",
      "[Epoch 18/20] Validation: 100%|██████████| 208/208 [01:22<00:00,  2.52it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0092 || Valid Loss : 0.2847 | Valid Accuracy : 92.7580%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 19/20] Training: 100%|██████████| 829/829 [12:35<00:00,  1.10it/s]\n",
      "[Epoch 19/20] Validation: 100%|██████████| 208/208 [01:23<00:00,  2.48it/s]\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0309 || Valid Loss : 0.2954 | Valid Accuracy : 92.4261%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 20/20] Training: 100%|██████████| 829/829 [12:38<00:00,  1.09it/s]\n",
      "[Epoch 20/20] Validation: 100%|██████████| 208/208 [01:22<00:00,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.0130 || Valid Loss : 0.2965 | Valid Accuracy : 92.6222%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = BaseModel(num_classes=len(class_names)).to(device)\n",
    "best_logloss = float('inf')\n",
    "\n",
    "# 손실 함수\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "\n",
    "# 학습 및 검증 루프\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # LogLoss\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_names))))\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f} | Valid Accuracy : {val_accuracy:.4f}%\")\n",
    "\n",
    "    # Best model 저장\n",
    "    if val_logloss < best_logloss:\n",
    "        best_logloss = val_logloss\n",
    "        torch.save(model.state_dict(), f'best_model.pth')\n",
    "        print(f\"📦 Best model saved at epoch {epoch+1} (logloss: {val_logloss:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_root, transform=val_transform, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\miniconda3\\envs\\dacon_cv_310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 저장된 모델 로드\n",
    "model = BaseModel(num_classes=len(class_names))\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# 추론\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # 각 배치의 확률을 리스트로 변환\n",
    "        for prob in probs.cpu():  # prob: (num_classes,)\n",
    "            result = {\n",
    "                class_names[i]: prob[i].item()\n",
    "                for i in range(len(class_names))\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "pred = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data/sample_submission.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 'ID' 컬럼을 제외한 클래스 컬럼 정렬\n",
    "class_columns = submission.columns[1:]\n",
    "pred = pred[class_columns]\n",
    "\n",
    "submission[class_columns] = pred.values\n",
    "submission.to_csv('0525_submission.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon_cv_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
