{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96170cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dbb5630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/origin/train.csv\"\n",
    "test_path = \"../data/origin/test.csv\"\n",
    "building_path = \"../data/origin/building_info.csv\"\n",
    "submission_path = \"../data/origin/sample_submission.csv\"\n",
    "\n",
    "ko2en_dict = {\n",
    " '건물번호': 'b_num',\n",
    " '일시': 'date',\n",
    " '기온(°C)': 'tmp',\n",
    " '강수량(mm)': 'rain',\n",
    " '풍속(m/s)': 'wind',\n",
    " '습도(%)': 'hum',\n",
    " '일조(hr)': 'sunshine',\n",
    " '일사(MJ/m2)': 'solar',\n",
    " '전력소비량(kWh)': 'power_consumption',\n",
    " '건물유형': 'b_type',\n",
    " '연면적(m2)': 'total_area',\n",
    " '냉방면적(m2)': 'cooling_area',\n",
    " '태양광용량(kW)': 'solar_capacity',\n",
    " 'ESS저장용량(kWh)': 'ess_capacity',\n",
    " 'PCS용량(kW)': 'pcs_capacity',\n",
    "}\n",
    "\n",
    "change_name = ['hotel', 'commercial', 'hospital', 'school', 'etc', 'apart', 'research', 'store', 'idc','public']\n",
    "\n",
    "train = pd.read_csv(train_path, encoding='utf-8')\n",
    "test = pd.read_csv(test_path, encoding='utf-8')\n",
    "building = pd.read_csv(building_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6cef18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dataframe_columns(df, mapping_dict):\n",
    "    return df.rename(columns=mapping_dict).copy()\n",
    "\n",
    "def add_time(df):\n",
    "    df['datetime'] = pd.to_datetime(df['date'], format='%Y%m%d %H')\n",
    "    df['datetime'] = df['datetime'].dt.strftime(\"%Y-%m-%d %H\")\n",
    "    # df.set_index('datetime', inplace=True)\n",
    "    return df\n",
    "\n",
    "def outlier_process(df, threshold=2.0):\n",
    "    '''이상치 처리 메서드'''\n",
    "    df = df.copy()\n",
    "    for key, group in df.groupby(\"b_num\"):\n",
    "        idx = group.index\n",
    "        vals = group[\"power_consumption\"].to_numpy()\n",
    "        for i in range(1, len(vals) - 1):\n",
    "            if vals[i-1] == 0: \n",
    "                continue\n",
    "            ratio = vals[i] / vals[i-1]\n",
    "            if ratio >= threshold or ratio <= 1/threshold:\n",
    "                vals[i] = (vals[i-1] + vals[i+1]) / 2\n",
    "        df.loc[idx, \"power_consumption\"] = vals\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "75bab7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_date_time</th>\n",
       "      <th>b_num</th>\n",
       "      <th>date</th>\n",
       "      <th>tmp</th>\n",
       "      <th>rain</th>\n",
       "      <th>wind</th>\n",
       "      <th>hum</th>\n",
       "      <th>power_consumption</th>\n",
       "      <th>datetime</th>\n",
       "      <th>b_type</th>\n",
       "      <th>total_area</th>\n",
       "      <th>cooling_area</th>\n",
       "      <th>solar_capacity</th>\n",
       "      <th>ess_capacity</th>\n",
       "      <th>pcs_capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_20240601 00</td>\n",
       "      <td>1</td>\n",
       "      <td>20240601 00</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>5794.80</td>\n",
       "      <td>2024-06-01 00</td>\n",
       "      <td>hotel</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_20240601 01</td>\n",
       "      <td>1</td>\n",
       "      <td>20240601 01</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>82.0</td>\n",
       "      <td>5591.85</td>\n",
       "      <td>2024-06-01 01</td>\n",
       "      <td>hotel</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_20240601 02</td>\n",
       "      <td>1</td>\n",
       "      <td>20240601 02</td>\n",
       "      <td>18.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>5338.17</td>\n",
       "      <td>2024-06-01 02</td>\n",
       "      <td>hotel</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_20240601 03</td>\n",
       "      <td>1</td>\n",
       "      <td>20240601 03</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>4554.42</td>\n",
       "      <td>2024-06-01 03</td>\n",
       "      <td>hotel</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_20240601 04</td>\n",
       "      <td>1</td>\n",
       "      <td>20240601 04</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>81.0</td>\n",
       "      <td>3602.25</td>\n",
       "      <td>2024-06-01 04</td>\n",
       "      <td>hotel</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_date_time  b_num         date   tmp  rain  wind   hum  \\\n",
       "0  1_20240601 00      1  20240601 00  18.3   0.0   2.6  82.0   \n",
       "1  1_20240601 01      1  20240601 01  18.3   0.0   2.7  82.0   \n",
       "2  1_20240601 02      1  20240601 02  18.1   0.0   2.6  80.0   \n",
       "3  1_20240601 03      1  20240601 03  18.0   0.0   2.6  81.0   \n",
       "4  1_20240601 04      1  20240601 04  17.8   0.0   1.3  81.0   \n",
       "\n",
       "   power_consumption       datetime b_type  total_area  cooling_area  \\\n",
       "0            5794.80  2024-06-01 00  hotel    82912.71       77586.0   \n",
       "1            5591.85  2024-06-01 01  hotel    82912.71       77586.0   \n",
       "2            5338.17  2024-06-01 02  hotel    82912.71       77586.0   \n",
       "3            4554.42  2024-06-01 03  hotel    82912.71       77586.0   \n",
       "4            3602.25  2024-06-01 04  hotel    82912.71       77586.0   \n",
       "\n",
       "  solar_capacity ess_capacity pcs_capacity  \n",
       "0              0            0            0  \n",
       "1              0            0            0  \n",
       "2              0            0            0  \n",
       "3              0            0            0  \n",
       "4              0            0            0  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = rename_dataframe_columns(train, ko2en_dict)\n",
    "test_df = rename_dataframe_columns(test, ko2en_dict)\n",
    "building_info_df = rename_dataframe_columns(building, ko2en_dict)\n",
    "\n",
    "train_df = add_time(train_df)\n",
    "test_df = add_time(test_df)\n",
    "\n",
    "train_merge = pd.merge(train_df, building_info_df, on='b_num', how='left')\n",
    "test_merge = pd.merge(test_df, building_info_df, on='b_num', how='left')\n",
    "\n",
    "btypes = list(building_info_df['b_type'].unique())\n",
    "type_map = {bt: change_name[i] for i, bt in enumerate(btypes)}\n",
    "train_merge['b_type'] = train_merge['b_type'].apply(lambda x : type_map[x])\n",
    "test_merge['b_type'] = test_merge['b_type'].apply(lambda x : type_map[x])\n",
    "\n",
    "train_merge = outlier_process(train_merge)\n",
    "\n",
    "train_merge = train_merge.replace(\"-\", 0)\n",
    "test_merge = test_merge.replace(\"-\", 0)\n",
    "\n",
    "train_merge.drop(['sunshine', 'solar'], axis=1, inplace=True)\n",
    "train_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "58420243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalering(df: pd.DataFrame, exclude_cols, scaler,fit):\n",
    "    '''Scalering 적용'''\n",
    "    target_cols = [i for i in df.columns if i not in exclude_cols]\n",
    "    if fit:\n",
    "        df[target_cols] = scaler.fit_transform(df[target_cols])\n",
    "    else:\n",
    "        df[target_cols] = scaler.transform(df[target_cols])\n",
    "    return df\n",
    "\n",
    "def train_validation_split(df, seq_len, ratio=0.8):\n",
    "    '''학습 검증 데이터 분리'''\n",
    "    train_size = int(df.shape[0] * ratio)\n",
    "\n",
    "    train_set = df.iloc[:train_size]\n",
    "    test_set = df.iloc[train_size - seq_len:]\n",
    "    return train_set, test_set\n",
    "\n",
    "def make_dataset(data, seq_length):\n",
    "    '''LSTM 모델 학습을 위한 데이터 셋 구축'''\n",
    "    dataX, dataY = [], []\n",
    "\n",
    "    if 'power_consumption' in data.columns:\n",
    "        for i in range(0, data.shape[0] - seq_length):\n",
    "            x = data.iloc[i:i+seq_length].drop(columns=['power_consumption']).values\n",
    "            y = data.iloc[i+seq_length]['power_consumption']\n",
    "            dataX.append(x)\n",
    "            dataY.append(y)\n",
    "\n",
    "        return np.array(dataX), np.array(dataY).reshape(-1, 1)\n",
    "    else:\n",
    "        for i in range(0, data.shape[0] - seq_length):\n",
    "            x = data.iloc[i:i+seq_length].values   \n",
    "            dataX.append(x)\n",
    "        return np.array(dataX), None \n",
    "\n",
    "def smape_loss(y_true, y_pred):\n",
    "    \"\"\"SMAPE 계산\"\"\"\n",
    "    return 100 * torch.mean(\n",
    "        2 * torch.abs(y_pred - y_true) / (torch.abs(y_true) + torch.abs(y_pred) + 1e-9)\n",
    "    )\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(\n",
    "            self,input_dim,hidden_dim,output_dim,seq_length,layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 속성 저장\n",
    "        self.input_dim = input_dim \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.seq_length = seq_length \n",
    "        self.layers = layers \n",
    "\n",
    "        # 레이어(batch size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.input_dim, \n",
    "            self.hidden_dim,\n",
    "            num_layers=self.layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.output_dim, bias=True)\n",
    "\n",
    "    def reset_hidden_state(self):\n",
    "        \"\"\"LSTM 학습 초기화하는 함수\"\"\"\n",
    "        self.hidden = (\n",
    "            torch.zeros(self.layers, self.seq_length, self.hidden_dim),\n",
    "            torch.zeros(self.layers, self.seq_length, self.hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x[:,-1])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1f1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hotel is training]\n",
      "[Epoch 1/50] Train Loss: 21747430.8098, Train SMAPE: 189.8904, Val Loss: 3646786.8796, Val SMAPE: 181.6964\n",
      "[Epoch 2/50] Train Loss: 21202662.7537, Train SMAPE: 172.6993, Val Loss: 3374129.4192, Val SMAPE: 166.1855\n",
      "[Epoch 3/50] Train Loss: 20697630.8294, Train SMAPE: 158.4260, Val Loss: 3120355.5198, Val SMAPE: 152.3655\n",
      "[Epoch 4/50] Train Loss: 20215439.5622, Train SMAPE: 146.0171, Val Loss: 2882370.5152, Val SMAPE: 139.8676\n",
      "[Epoch 5/50] Train Loss: 19752627.9604, Train SMAPE: 135.0550, Val Loss: 2658826.7591, Val SMAPE: 128.4744\n",
      "[Epoch 6/50] Train Loss: 19307553.5945, Train SMAPE: 125.3058, Val Loss: 2448855.5861, Val SMAPE: 118.0277\n",
      "[Epoch 7/50] Train Loss: 18879154.0461, Train SMAPE: 116.6614, Val Loss: 2251791.2957, Val SMAPE: 108.4039\n",
      "[Epoch 8/50] Train Loss: 18466634.9138, Train SMAPE: 109.2015, Val Loss: 2067078.3243, Val SMAPE: 99.5031\n",
      "[Epoch 9/50] Train Loss: 18069353.0527, Train SMAPE: 102.7587, Val Loss: 1894230.3251, Val SMAPE: 91.2431\n",
      "[Epoch 10/50] Train Loss: 17686765.4533, Train SMAPE: 97.3145, Val Loss: 1732808.1357, Val SMAPE: 83.5550\n",
      "[Epoch 11/50] Train Loss: 17318397.7196, Train SMAPE: 92.6767, Val Loss: 1582405.2752, Val SMAPE: 76.3803\n",
      "[Epoch 12/50] Train Loss: 16963822.7862, Train SMAPE: 88.7244, Val Loss: 1442638.4585, Val SMAPE: 69.6738\n",
      "[Epoch 13/50] Train Loss: 16622651.9733, Train SMAPE: 85.4270, Val Loss: 1313143.0741, Val SMAPE: 63.4343\n",
      "[Epoch 14/50] Train Loss: 16294526.0954, Train SMAPE: 82.6903, Val Loss: 1193566.3672, Val SMAPE: 57.7009\n",
      "[Epoch 15/50] Train Loss: 15979101.8953, Train SMAPE: 80.3925, Val Loss: 1083564.7631, Val SMAPE: 52.5946\n",
      "[Epoch 16/50] Train Loss: 15676057.0007, Train SMAPE: 78.5376, Val Loss: 982801.8990, Val SMAPE: 48.1254\n",
      "[Epoch 17/50] Train Loss: 15385077.2385, Train SMAPE: 77.1428, Val Loss: 890947.2637, Val SMAPE: 44.2366\n",
      "[Epoch 18/50] Train Loss: 15105860.2485, Train SMAPE: 76.1278, Val Loss: 807669.4920, Val SMAPE: 40.9038\n",
      "[Epoch 19/50] Train Loss: 14838097.6933, Train SMAPE: 75.3698, Val Loss: 732641.1076, Val SMAPE: 38.0061\n",
      "[Epoch 20/50] Train Loss: 14581489.6385, Train SMAPE: 74.7862, Val Loss: 665537.5379, Val SMAPE: 35.5481\n",
      "[Epoch 21/50] Train Loss: 14335735.6866, Train SMAPE: 74.3180, Val Loss: 606031.5612, Val SMAPE: 33.4787\n",
      "[Epoch 22/50] Train Loss: 14100538.4414, Train SMAPE: 73.9400, Val Loss: 553803.7833, Val SMAPE: 31.7783\n",
      "[Epoch 23/50] Train Loss: 13875603.9923, Train SMAPE: 73.6344, Val Loss: 508532.1026, Val SMAPE: 30.4117\n",
      "[Epoch 24/50] Train Loss: 13660634.8262, Train SMAPE: 73.4048, Val Loss: 469894.5091, Val SMAPE: 29.3565\n",
      "[Epoch 25/50] Train Loss: 13455339.1281, Train SMAPE: 73.2447, Val Loss: 437586.5361, Val SMAPE: 28.5959\n",
      "[Epoch 26/50] Train Loss: 13259411.2035, Train SMAPE: 73.1375, Val Loss: 411286.2091, Val SMAPE: 28.0608\n",
      "[Epoch 27/50] Train Loss: 13072560.2285, Train SMAPE: 73.0680, Val Loss: 390685.4305, Val SMAPE: 27.7000\n",
      "[Epoch 28/50] Train Loss: 12894492.8899, Train SMAPE: 73.0346, Val Loss: 375477.6482, Val SMAPE: 27.4770\n",
      "[Epoch 29/50] Train Loss: 12724909.0812, Train SMAPE: 73.0387, Val Loss: 365361.9143, Val SMAPE: 27.3716\n",
      "[Epoch 30/50] Train Loss: 12563517.9940, Train SMAPE: 73.0769, Val Loss: 360041.9839, Val SMAPE: 27.3818\n",
      "[Epoch 31/50] Train Loss: 12410025.7769, Train SMAPE: 73.1451, Val Loss: 359227.8142, Val SMAPE: 27.4910\n",
      "[Epoch 32/50] Train Loss: 12264145.4471, Train SMAPE: 73.2435, Val Loss: 362635.6159, Val SMAPE: 27.6979\n",
      "[Epoch 33/50] Train Loss: 12125590.4366, Train SMAPE: 73.3671, Val Loss: 369988.5035, Val SMAPE: 27.9853\n",
      "[Epoch 34/50] Train Loss: 11994082.3269, Train SMAPE: 73.5088, Val Loss: 381015.7555, Val SMAPE: 28.3544\n",
      "[Epoch 35/50] Train Loss: 11869340.7076, Train SMAPE: 73.6616, Val Loss: 395455.2194, Val SMAPE: 28.7830\n",
      "[Epoch 36/50] Train Loss: 11751088.0536, Train SMAPE: 73.8193, Val Loss: 413055.0143, Val SMAPE: 29.2632\n",
      "[Epoch 37/50] Train Loss: 11639051.5537, Train SMAPE: 73.9833, Val Loss: 433565.6761, Val SMAPE: 29.7830\n",
      "[Epoch 38/50] Train Loss: 11532975.2558, Train SMAPE: 74.1527, Val Loss: 456752.9390, Val SMAPE: 30.3399\n",
      "[Epoch 39/50] Train Loss: 11432594.8530, Train SMAPE: 74.3231, Val Loss: 482389.4741, Val SMAPE: 30.9299\n",
      "[Epoch 40/50] Train Loss: 11337659.7238, Train SMAPE: 74.4949, Val Loss: 510255.4781, Val SMAPE: 31.5530\n",
      "[Epoch 41/50] Train Loss: 11247922.1984, Train SMAPE: 74.6648, Val Loss: 540133.7107, Val SMAPE: 32.2031\n",
      "[Epoch 42/50] Train Loss: 11163145.8076, Train SMAPE: 74.8322, Val Loss: 571842.4880, Val SMAPE: 32.8718\n",
      "[Epoch 43/50] Train Loss: 11083099.4046, Train SMAPE: 74.9973, Val Loss: 605167.3388, Val SMAPE: 33.5563\n",
      "[Epoch 44/50] Train Loss: 11007547.8725, Train SMAPE: 75.1585, Val Loss: 639940.6143, Val SMAPE: 34.2537\n",
      "[Epoch 45/50] Train Loss: 10936279.9896, Train SMAPE: 75.3150, Val Loss: 675998.0789, Val SMAPE: 34.9646\n",
      "[Epoch 46/50] Train Loss: 10869082.6798, Train SMAPE: 75.4656, Val Loss: 713138.4657, Val SMAPE: 35.6850\n",
      "[Epoch 47/50] Train Loss: 10805749.1903, Train SMAPE: 75.6107, Val Loss: 751371.1250, Val SMAPE: 36.4135\n",
      "[Epoch 48/50] Train Loss: 10746085.7720, Train SMAPE: 75.7523, Val Loss: 790253.5587, Val SMAPE: 37.1412\n",
      "[Epoch 49/50] Train Loss: 10689901.4332, Train SMAPE: 75.8900, Val Loss: 829993.8392, Val SMAPE: 37.8655\n",
      "[Epoch 50/50] Train Loss: 10637015.5022, Train SMAPE: 76.0228, Val Loss: 870165.4200, Val SMAPE: 38.5791\n",
      "[hotel] Test 예측 shape: (1656,)\n",
      "========================================================================================================================================================================================================\n",
      "[commercial is training]\n",
      "[Epoch 1/50] Train Loss: 9639628.8464, Train SMAPE: 191.7568, Val Loss: 1829509.4200, Val SMAPE: 167.9802\n",
      "[Epoch 2/50] Train Loss: 9099121.4896, Train SMAPE: 174.1765, Val Loss: 1684334.9535, Val SMAPE: 147.3226\n",
      "[Epoch 3/50] Train Loss: 8623414.6837, Train SMAPE: 160.3174, Val Loss: 1557318.8514, Val SMAPE: 130.2953\n",
      "[Epoch 4/50] Train Loss: 8178012.8677, Train SMAPE: 148.1142, Val Loss: 1445464.6553, Val SMAPE: 115.8510\n",
      "[Epoch 5/50] Train Loss: 7757568.2982, Train SMAPE: 137.1545, Val Loss: 1347322.8690, Val SMAPE: 103.3750\n",
      "[Epoch 6/50] Train Loss: 7359560.6329, Train SMAPE: 127.2017, Val Loss: 1261946.0987, Val SMAPE: 92.5501\n",
      "[Epoch 7/50] Train Loss: 6982375.5164, Train SMAPE: 118.0928, Val Loss: 1188646.8166, Val SMAPE: 84.3658\n",
      "[Epoch 8/50] Train Loss: 6624825.3183, Train SMAPE: 109.7064, Val Loss: 1126983.8567, Val SMAPE: 78.0461\n",
      "[Epoch 9/50] Train Loss: 6285968.7106, Train SMAPE: 101.9882, Val Loss: 1076178.2844, Val SMAPE: 72.6601\n",
      "[Epoch 10/50] Train Loss: 5965022.3148, Train SMAPE: 95.5643, Val Loss: 1035870.0870, Val SMAPE: 68.2909\n",
      "[Epoch 11/50] Train Loss: 5661320.6282, Train SMAPE: 90.0814, Val Loss: 1005588.3599, Val SMAPE: 65.0640\n",
      "[Epoch 12/50] Train Loss: 5374276.4907, Train SMAPE: 85.0845, Val Loss: 984813.9856, Val SMAPE: 63.1333\n",
      "[Epoch 13/50] Train Loss: 5103359.9094, Train SMAPE: 80.4450, Val Loss: 973167.0591, Val SMAPE: 62.6469\n",
      "[Epoch 14/50] Train Loss: 4848084.0348, Train SMAPE: 76.1581, Val Loss: 970183.2281, Val SMAPE: 63.4300\n",
      "[Epoch 15/50] Train Loss: 4607991.0736, Train SMAPE: 72.2428, Val Loss: 975448.1494, Val SMAPE: 65.6457\n",
      "[Epoch 16/50] Train Loss: 4382635.2292, Train SMAPE: 68.6544, Val Loss: 988506.4449, Val SMAPE: 68.4578\n",
      "[Epoch 17/50] Train Loss: 4171578.6826, Train SMAPE: 65.3672, Val Loss: 1008877.6237, Val SMAPE: 71.2564\n",
      "[Epoch 18/50] Train Loss: 3974389.0894, Train SMAPE: 62.3916, Val Loss: 1036089.9499, Val SMAPE: 73.8822\n",
      "[Epoch 19/50] Train Loss: 3790623.2569, Train SMAPE: 59.7240, Val Loss: 1069639.7850, Val SMAPE: 76.3383\n",
      "[Epoch 20/50] Train Loss: 3619830.3734, Train SMAPE: 57.3228, Val Loss: 1109015.1454, Val SMAPE: 78.6340\n",
      "[Epoch 21/50] Train Loss: 3461549.8185, Train SMAPE: 55.1414, Val Loss: 1153688.9815, Val SMAPE: 80.7802\n",
      "[Epoch 22/50] Train Loss: 3315297.6030, Train SMAPE: 53.1522, Val Loss: 1203124.1139, Val SMAPE: 82.7751\n",
      "[Epoch 23/50] Train Loss: 3180583.5746, Train SMAPE: 51.3422, Val Loss: 1256766.3704, Val SMAPE: 84.6351\n",
      "[Epoch 24/50] Train Loss: 3056894.1313, Train SMAPE: 49.7831, Val Loss: 1313978.3544, Val SMAPE: 86.3766\n",
      "[Epoch 25/50] Train Loss: 2943705.0383, Train SMAPE: 48.5756, Val Loss: 1374332.4204, Val SMAPE: 88.0127\n",
      "[Epoch 26/50] Train Loss: 2840478.6106, Train SMAPE: 47.5957, Val Loss: 1437264.5785, Val SMAPE: 89.5476\n",
      "[Epoch 27/50] Train Loss: 2746661.7995, Train SMAPE: 46.7451, Val Loss: 1502208.3963, Val SMAPE: 90.9821\n",
      "[Epoch 28/50] Train Loss: 2661697.7666, Train SMAPE: 46.0051, Val Loss: 1568633.0640, Val SMAPE: 92.3228\n",
      "[Epoch 29/50] Train Loss: 2585018.7708, Train SMAPE: 45.3540, Val Loss: 1636024.9825, Val SMAPE: 93.5739\n",
      "[Epoch 30/50] Train Loss: 2516068.7499, Train SMAPE: 44.7736, Val Loss: 1703892.3392, Val SMAPE: 94.7437\n",
      "[Epoch 31/50] Train Loss: 2454280.1047, Train SMAPE: 44.2599, Val Loss: 1771792.0267, Val SMAPE: 95.8395\n",
      "[Epoch 32/50] Train Loss: 2399108.6962, Train SMAPE: 43.8098, Val Loss: 1839473.4413, Val SMAPE: 96.8683\n",
      "[Epoch 33/50] Train Loss: 2350005.8645, Train SMAPE: 43.4107, Val Loss: 1906282.2348, Val SMAPE: 97.8282\n",
      "[Epoch 34/50] Train Loss: 2306451.4439, Train SMAPE: 43.0569, Val Loss: 1971828.6905, Val SMAPE: 98.7184\n",
      "[Epoch 35/50] Train Loss: 2267946.5455, Train SMAPE: 42.7396, Val Loss: 2035902.5381, Val SMAPE: 99.5438\n",
      "[Epoch 36/50] Train Loss: 2234009.4968, Train SMAPE: 42.4597, Val Loss: 2098297.1875, Val SMAPE: 100.3108\n",
      "[Epoch 37/50] Train Loss: 2204183.3905, Train SMAPE: 42.2094, Val Loss: 2158779.1814, Val SMAPE: 101.0215\n",
      "[Epoch 38/50] Train Loss: 2178046.8545, Train SMAPE: 41.9810, Val Loss: 2217239.5655, Val SMAPE: 101.6864\n",
      "[Epoch 39/50] Train Loss: 2155203.5747, Train SMAPE: 41.7716, Val Loss: 2273367.9466, Val SMAPE: 102.3244\n",
      "[Epoch 40/50] Train Loss: 2135291.0587, Train SMAPE: 41.5790, Val Loss: 2327244.2515, Val SMAPE: 102.9037\n",
      "[Epoch 41/50] Train Loss: 2213259.2305, Train SMAPE: 42.8718, Val Loss: 2218223.5610, Val SMAPE: 103.4065\n",
      "[Epoch 42/50] Train Loss: 2154272.8012, Train SMAPE: 41.7638, Val Loss: 2272002.6494, Val SMAPE: 104.1117\n",
      "[Epoch 43/50] Train Loss: 2113920.1626, Train SMAPE: 41.1408, Val Loss: 2491203.1090, Val SMAPE: 107.1567\n",
      "[Epoch 44/50] Train Loss: 2074087.7974, Train SMAPE: 40.8812, Val Loss: 2533839.1341, Val SMAPE: 107.5296\n",
      "[Epoch 45/50] Train Loss: 2065010.5501, Train SMAPE: 40.7566, Val Loss: 2584049.8872, Val SMAPE: 108.9287\n",
      "[Epoch 46/50] Train Loss: 2057206.7364, Train SMAPE: 40.6421, Val Loss: 2621306.4352, Val SMAPE: 109.2775\n",
      "[Epoch 47/50] Train Loss: 2137669.2213, Train SMAPE: 41.9482, Val Loss: 2455511.5960, Val SMAPE: 106.3242\n",
      "[Epoch 48/50] Train Loss: 2173539.5342, Train SMAPE: 42.4348, Val Loss: 2296896.1982, Val SMAPE: 105.7303\n",
      "[Epoch 49/50] Train Loss: 2321457.4557, Train SMAPE: 44.3738, Val Loss: 2023464.6364, Val SMAPE: 103.0870\n",
      "[Epoch 50/50] Train Loss: 2334494.1749, Train SMAPE: 44.2675, Val Loss: 2020533.4131, Val SMAPE: 108.5275\n",
      "[commercial] Test 예측 shape: (1656,)\n",
      "========================================================================================================================================================================================================\n",
      "[hospital is training]\n",
      "[Epoch 1/50] Train Loss: 52872371.8759, Train SMAPE: 196.4626, Val Loss: 6663713.8294, Val SMAPE: 191.8220\n",
      "[Epoch 2/50] Train Loss: 52318979.5889, Train SMAPE: 187.8782, Val Loss: 6370603.6757, Val SMAPE: 182.0352\n",
      "[Epoch 3/50] Train Loss: 51741061.0185, Train SMAPE: 180.2286, Val Loss: 6084334.9155, Val SMAPE: 172.7591\n",
      "[Epoch 4/50] Train Loss: 51169674.0938, Train SMAPE: 172.9754, Val Loss: 5805931.7669, Val SMAPE: 163.9820\n",
      "[Epoch 5/50] Train Loss: 50607603.3012, Train SMAPE: 166.1024, Val Loss: 5536098.4527, Val SMAPE: 155.6845\n",
      "[Epoch 6/50] Train Loss: 50055910.1881, Train SMAPE: 159.5822, Val Loss: 5275062.6588, Val SMAPE: 147.8362\n",
      "[Epoch 7/50] Train Loss: 49514989.4477, Train SMAPE: 153.3905, Val Loss: 5022849.3826, Val SMAPE: 140.4047\n",
      "[Epoch 8/50] Train Loss: 48984952.5923, Train SMAPE: 147.5046, Val Loss: 4779390.7720, Val SMAPE: 133.3588\n",
      "[Epoch 9/50] Train Loss: 48465776.1995, Train SMAPE: 141.9005, Val Loss: 4544574.0093, Val SMAPE: 126.6694\n",
      "[Epoch 10/50] Train Loss: 47957372.8061, Train SMAPE: 136.5568, Val Loss: 4318261.7694, Val SMAPE: 120.3102\n",
      "[Epoch 11/50] Train Loss: 47459617.3452, Train SMAPE: 131.4550, Val Loss: 4100308.7137, Val SMAPE: 114.2570\n",
      "[Epoch 12/50] Train Loss: 46972371.2116, Train SMAPE: 126.5790, Val Loss: 3890561.8209, Val SMAPE: 108.4882\n",
      "[Epoch 13/50] Train Loss: 46495486.9069, Train SMAPE: 121.9134, Val Loss: 3688867.9286, Val SMAPE: 102.9840\n",
      "[Epoch 14/50] Train Loss: 46028818.7181, Train SMAPE: 117.4441, Val Loss: 3495073.7209, Val SMAPE: 97.7266\n",
      "[Epoch 15/50] Train Loss: 45572214.7623, Train SMAPE: 113.1600, Val Loss: 3309028.4096, Val SMAPE: 92.6998\n",
      "[Epoch 16/50] Train Loss: 45125527.7870, Train SMAPE: 109.0492, Val Loss: 3130578.1909, Val SMAPE: 87.8888\n",
      "[Epoch 17/50] Train Loss: 44688614.1384, Train SMAPE: 105.1071, Val Loss: 2959576.2498, Val SMAPE: 83.2802\n",
      "[Epoch 18/50] Train Loss: 44261331.3477, Train SMAPE: 101.3559, Val Loss: 2795871.9854, Val SMAPE: 78.8631\n",
      "[Epoch 19/50] Train Loss: 43843540.1588, Train SMAPE: 97.8042, Val Loss: 2639322.9297, Val SMAPE: 74.6439\n",
      "[Epoch 20/50] Train Loss: 43435106.0283, Train SMAPE: 94.4282, Val Loss: 2489777.4348, Val SMAPE: 70.6485\n",
      "[Epoch 21/50] Train Loss: 43035880.0612, Train SMAPE: 91.1993, Val Loss: 2347091.9618, Val SMAPE: 66.9089\n",
      "[Epoch 22/50] Train Loss: 42645735.0367, Train SMAPE: 88.1234, Val Loss: 2211122.5436, Val SMAPE: 63.4163\n",
      "[Epoch 23/50] Train Loss: 42264534.1259, Train SMAPE: 85.2112, Val Loss: 2081723.9153, Val SMAPE: 60.1378\n"
     ]
    }
   ],
   "source": [
    "submission_result = {}\n",
    "drop_cols = ['num_date_time', 'b_num', 'date', 'b_type','total_area']\n",
    "exclude_cols = []\n",
    "seq_length = 24\n",
    "batch_size = 50\n",
    "scaler = StandardScaler()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "epochs = 50\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "for name in change_name:\n",
    "    train_dict[name] = train_merge[train_merge['b_type'] == name].set_index('datetime')\n",
    "    test_dict[name] = test_merge[test_merge['b_type'] == name].set_index('datetime')\n",
    "\n",
    "    train_dict[name].drop(drop_cols, axis=1, inplace=True)\n",
    "    test_dict[name].drop(drop_cols, axis=1, inplace=True)\n",
    "    \n",
    "    train_ex = ['power_consumption']\n",
    "    train_dict[name] = scalering(train_dict[name],train_ex,scaler, fit = True)\n",
    "    test_dict[name] = scalering(test_dict[name],exclude_cols,scaler, fit = False)\n",
    "\n",
    "    tr, vr = train_validation_split(train_dict[name], seq_length)\n",
    "    X_tr, y_tr = make_dataset(tr,seq_length)\n",
    "    X_vr, y_vr = make_dataset(vr,seq_length)\n",
    "    X_test, _ = make_dataset(test_dict[name], seq_length)\n",
    "\n",
    "    train_X_ts = torch.FloatTensor(X_tr)\n",
    "    train_y_ts = torch.FloatTensor(y_tr)\n",
    "    test_X_ts = torch.FloatTensor(X_test)\n",
    "\n",
    "    val_X_ts = torch.FloatTensor(X_vr)\n",
    "    val_y_ts = torch.FloatTensor(y_vr)\n",
    "\n",
    "    dataset = TensorDataset(train_X_ts, train_y_ts)\n",
    "    dataset_val = TensorDataset(val_X_ts, val_y_ts)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    val_loader = DataLoader(dataset_val, batch_size=batch_size)\n",
    "\n",
    "    input_dim = train_X_ts.shape[2]\n",
    "    model = LSTM(\n",
    "        input_dim = input_dim,\n",
    "        hidden_dim = 30,\n",
    "        output_dim = 1,\n",
    "        seq_length = seq_length,\n",
    "        layers = 1\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    loss_history = []\n",
    "    print(f\"[{name} is training]\")\n",
    "    for epoch in range(epochs):\n",
    "        # -------- Train --------\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_smape = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_smape += smape_loss(y_batch, outputs).item()\n",
    "\n",
    "        # -------- Validation --------\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_smape = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_smape += smape_loss(y_batch, outputs).item()\n",
    "\n",
    "        \n",
    "        print(f\"[Epoch {epoch+1}/{epochs}] \"\n",
    "            f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "            f\"Train SMAPE: {train_smape/len(train_loader):.4f}, \"\n",
    "            f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "            f\"Val SMAPE: {val_smape/len(val_loader):.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_X_ts), batch_size):\n",
    "            batch = test_X_ts[i:i+batch_size]\n",
    "            output = model(batch)\n",
    "            preds.extend(output.cpu().numpy())\n",
    "\n",
    "    preds = np.array(preds).reshape(-1)\n",
    "\n",
    "    # 결과 저장\n",
    "    submission_result[name] = preds\n",
    "    print(f\"[{name}] Test 예측 shape: {preds.shape}\")\n",
    "    print(\"=\" * 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98598e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission 불러오기\n",
    "sample_submission = pd.read_csv(submission_path)\n",
    "\n",
    "# 건물 유형별 예측 채워넣기\n",
    "for name in change_name:\n",
    "    sample_submission.loc[sample_submission['b_type'] == name, 'answer'] = submission_result[name]\n",
    "\n",
    "# 저장\n",
    "sample_submission.to_csv(\"../result/0816/LSTM.csv\", index=False)\n",
    "print(\"LSTM.csv 파일이 저장되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c775a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16296, 24, 13) (16296, 1)\n",
      "(4080, 24, 13) (4080, 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213d307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
